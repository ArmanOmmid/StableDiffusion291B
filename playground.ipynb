{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_NamespacePath(['/Users/armanommid/Code/CSE/CSE291B/Project/taming', '/Users/armanommid/Code/CSE/CSE291B/Project/taming', '/Users/armanommid/.Trash/thing/src/taming-transformers/taming'])\n"
     ]
    }
   ],
   "source": [
    "import argparse, os, sys, datetime, glob, importlib, csv\n",
    "import pytorch_lightning\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ldm.models.autoencoder import AutoencoderKL\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "data_dir = os.path.join(dir_path, \"datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class StandardDataset(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, dataset_name, data_dir, batch_size=64, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dataset = getattr(torchvision.datasets, self.dataset_name)(self.data_dir, train=True, download=True)\n",
    "\n",
    "    def setup(self):\n",
    "        dataset = getattr(torchvision.datasets, self.dataset_name)(self.data_dir, train=True, download=False, transform=T.ToTensor())\n",
    "        total = len(dataset)\n",
    "        n_train = int(total * 0.8)\n",
    "        n_val = int(total - n_train)\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "dataset_name = \"CIFAR10\"\n",
    "dataset = StandardDataset(dataset_name, data_dir)\n",
    "dataset.prepare_data()\n",
    "dataset.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 64, 8, 8) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | encoder         | Encoder                | 42.6 M\n",
      "1 | decoder         | Decoder                | 61.2 M\n",
      "2 | loss            | LPIPSWithDiscriminator | 17.5 M\n",
      "3 | quant_conv      | Conv2d                 | 16.5 K\n",
      "4 | post_quant_conv | Conv2d                 | 4.2 K \n",
      "-----------------------------------------------------------\n",
      "106 M     Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "121 M     Total params\n",
      "485.448   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1250 [00:00<00:00, 1864.96it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1047: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = \"configs/autoencoder/autoencoder_kl_8x8x64.yaml\"\n",
    "config = OmegaConf.load(config)\n",
    "\n",
    "model = instantiate_from_config(config.model)\n",
    "model.learning_rate = config.model.base_learning_rate\n",
    "\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=3)\n",
    "\n",
    "trainer.fit(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
