{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'taming.modules.vqvae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/armanommid/Code/CSE/CSE291B/Project/playground.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE291B/Project/playground.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m random_split\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE291B/Project/playground.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE291B/Project/playground.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mautoencoder\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoencoderKL\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE291B/Project/playground.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m instantiate_from_config\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/armanommid/Code/CSE/CSE291B/Project/playground.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m dir_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetcwd()\n",
      "File \u001b[0;32m~/Code/CSE/CSE291B/Project/ldm/models/autoencoder.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcontextlib\u001b[39;00m \u001b[39mimport\u001b[39;00m contextmanager\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtaming\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvqvae\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mquantize\u001b[39;00m \u001b[39mimport\u001b[39;00m VectorQuantizer2 \u001b[39mas\u001b[39;00m VectorQuantizer\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdiffusionmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m Encoder, Decoder\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mldm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodules\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m DiagonalGaussianDistribution\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'taming.modules.vqvae'"
     ]
    }
   ],
   "source": [
    "!pip3 install -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers\n",
    "!pip3 install -e git+https://github.com/openai/CLIP.git@main#egg=clip\n",
    "\n",
    "import argparse, os, sys, datetime, glob, importlib, csv\n",
    "import pytorch_lightning\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ldm.models.autoencoder import AutoencoderKL\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "data_dir = os.path.join(dir_path, \"datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "class StandardDataset(pytorch_lightning.LightningDataModule):\n",
    "    def __init__(self, dataset_name, data_dir, batch_size=64, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def prepare_data(self):\n",
    "        dataset = getattr(torchvision.datasets, self.dataset_name)(self.data_dir, train=True, download=True)\n",
    "\n",
    "    def setup(self):\n",
    "        dataset = getattr(torchvision.datasets, self.dataset_name)(self.data_dir, train=True, download=False, transform=T.ToTensor())\n",
    "        total = len(dataset)\n",
    "        n_train = int(total * 0.8)\n",
    "        n_val = int(total - n_train)\n",
    "        self.train_dataset, self.val_dataset = random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "\n",
    "dataset_name = \"CIFAR10\"\n",
    "dataset = StandardDataset(dataset_name, data_dir)\n",
    "dataset.prepare_data()\n",
    "dataset.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 64, 8, 8) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name            | Type                   | Params\n",
      "-----------------------------------------------------------\n",
      "0 | encoder         | Encoder                | 42.6 M\n",
      "1 | decoder         | Decoder                | 61.2 M\n",
      "2 | loss            | LPIPSWithDiscriminator | 17.5 M\n",
      "3 | quant_conv      | Conv2d                 | 16.5 K\n",
      "4 | post_quant_conv | Conv2d                 | 4.2 K \n",
      "-----------------------------------------------------------\n",
      "106 M     Trainable params\n",
      "14.7 M    Non-trainable params\n",
      "121 M     Total params\n",
      "485.448   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1250 [00:00<00:00, 1864.96it/s]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armanommid/anaconda3/envs/ldm/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1047: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = \"configs/autoencoder/autoencoder_kl_8x8x64.yaml\"\n",
    "config = OmegaConf.load(config)\n",
    "\n",
    "model = instantiate_from_config(config.model)\n",
    "model.learning_rate = config.model.base_learning_rate\n",
    "\n",
    "trainer = pytorch_lightning.Trainer(max_epochs=3)\n",
    "\n",
    "trainer.fit(model, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
